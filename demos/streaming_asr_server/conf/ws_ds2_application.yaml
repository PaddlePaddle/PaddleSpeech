# This is the parameter configuration file for PaddleSpeech Serving.

#################################################################################
#                             SERVER SETTING                                    #
#################################################################################
host: 0.0.0.0
port: 8090

# The task format in the engin_list is: <speech task>_<engine type>
# task choices = ['asr_online-inference', 'asr_online-onnx']
# protocol = ['websocket'] (only one can be selected).
# websocket only support online engine type.
protocol: 'websocket'
engine_list: ['asr_online-inference']


#################################################################################
#                                ENGINE CONFIG                                  #
#################################################################################

################################### ASR #########################################
################### speech task: asr; engine_type: online-inference #######################
asr_online-inference:
    model_type: 'deepspeech2online_aishell'
    am_model:   # the pdmodel file of am static model [optional]
    am_params:  # the pdiparams file of am static model [optional]
    lang: 'zh'
    sample_rate: 16000
    cfg_path: 
    decode_method: 
    num_decoding_left_chunks: 
    force_yes: True
    device: 'cpu' # cpu or gpu:id

    am_predictor_conf:
        device:  # set 'gpu:id' or 'cpu'
        switch_ir_optim: True
        glog_info: False  # True -> print glog
        summary: True  # False -> do not show predictor config

    chunk_buffer_conf:
        frame_duration_ms: 80
        shift_ms: 40
        sample_rate: 16000
        sample_width: 2
        window_n: 7     # frame
        shift_n: 4      # frame
        window_ms: 20   # ms
        shift_ms: 10    # ms



################################### ASR #########################################
################### speech task: asr; engine_type: online-onnx #######################
asr_online-onnx:
    model_type: 'deepspeech2online_aishell'
    am_model:  # the pdmodel file of am static model [optional]
    am_params:  # the pdiparams file of am static model [optional]
    lang: 'zh'
    sample_rate: 16000
    cfg_path: 
    decode_method: 
    num_decoding_left_chunks: 
    force_yes: True
    device: 'cpu' # cpu or gpu:id

    # https://onnxruntime.ai/docs/api/python/api_summary.html#inferencesession
    am_predictor_conf:
        device: 'cpu' # set 'gpu:id' or 'cpu'
        graph_optimization_level: 0 
        intra_op_num_threads: 0 # Sets the number of threads used to parallelize the execution within nodes.
        inter_op_num_threads: 0 # Sets the number of threads used to parallelize the execution of the graph (across nodes).
        log_severity_level: 2   # Log severity level. Applies to session load, initialization, etc. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. Default is 2.
        log_verbosity_level: 0  # VLOG level if DEBUG build and session_log_severity_level is 0. Applies to session load, initialization, etc. Default is 0.

    chunk_buffer_conf:
        frame_duration_ms: 80
        shift_ms: 40
        sample_rate: 16000
        sample_width: 2
        window_n: 7     # frame
        shift_n: 4      # frame
        window_ms: 20   # ms
        shift_ms: 10    # ms
