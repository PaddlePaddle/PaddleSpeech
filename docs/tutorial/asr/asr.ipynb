{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ASR Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"600\" height=\"360\" src=\"work/source/asr-demo-1.mp4\">animation</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果为：\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as dp\n",
    "from IPython.display import HTML\n",
    "html_str = '''\n",
    "<video controls width=\"600\" height=\"360\" src=\"{}\">animation</video>\n",
    "'''.format(\"work/source/asr-demo-1.mp4\")\n",
    "dp.display(HTML(html_str))\n",
    "print (\"结果为：\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "尝试使用，可点击该链接：（hub链接）[](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 背景知识\n",
    "语音识别(ASR, Automatic Speech Recognition) 是一项从一段音频中提取出语言文字内容的任务。目前该技术已经广泛应用于我们的工作和生活当中，包括生活中使用手机的语音转写，工作上使用的会议记录等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 发展历史\n",
    "* 早期，生成模型流行阶段：GMM-HMM (上世纪90年代，2006以前)\n",
    "* 深度学习爆发初期： DNN，CTC[1] （2006）\n",
    "* RNN流行，Attention提出初期: RNN-T[2]（2013）, DeepSpeech[3](2014)， DeepSpeech2 [4](2016), LAS[5]（2016）\n",
    "* Attetion is all you need提出开始[6]: transformer[6]（2017），transformer-transducer[7]（2020） conformer[8] （2020）\n",
    "\n",
    "\n",
    "目前transformer和conformer是语音识别领域的主流模型，因此本教程采用了transformer作为讲解的主要内容，并在课后作业中步骤了coformer的相关练习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用Transformer进行语音识别的的基本流程\n",
    "<div align=center>\n",
    "<img src=\"work/source/transformer_asr_pipeline.png\" />\n",
    "</div>\n",
    "\n",
    "对于语音识别的流程，最为简单的描述就是：第一步特征提取模块获取音频的声学特征，接着第二部语音识别模型利用声学特征来获取识别结果。\n",
    "\n",
    "声学提取模块一般使用fbank特征，这在后续的章节中会有讲解。\n",
    "\n",
    "而对于语音识别模型，本课程使用的transformer语音识别模型主要分为2个部分，第一个部分是Encoder，第二个部分是Decoder。\n",
    "\n",
    "声学特征会首先进入Encoder，获取特征编码。然后Decoder会利用Encoder提取的特征编码得到预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage 0 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 安装 paddlespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Processing ./work/wheel_store/sentencepiece-0.1.96-cp37-cp37m-linux_x86_64.whl\n",
      "Installing collected packages: sentencepiece\n",
      "  Found existing installation: sentencepiece 0.1.85\n",
      "    Uninstalling sentencepiece-0.1.85:\n",
      "      Successfully uninstalled sentencepiece-0.1.85\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlespeech\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bc/c3/4829550a06df372d607018c75223b7710fba4ac8d2a82b966c4125d13cce/paddlespeech-0.1.0a1-py3-none-any.whl (723kB)\n",
      "\u001b[K     |████████████████████████████████| 727kB 4.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textgrid (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/9e/04fb27ec5ac287b203afd5b228bc7c4ec5b7d3d81c4422d57847e755b0cc/TextGrid-1.5-py3-none-any.whl\n",
      "Collecting praatio~=4.1 (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a8/22/23bf5679577df32988a6b442356c972a7da9e0edeb4785dd9a1e3d806a0a/praatio-4.4.0-py2.py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 29.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.21.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (5.7.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.6.3)\n",
      "Collecting jsonlines (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n",
      "Collecting editdistance (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/a6/97ed63a93277eea43b09dc607237b22fb0151165c7851abfc0edc1bda2de/editdistance-0.6.0-cp37-cp37m-manylinux2010_x86_64.whl (285kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pypinyin (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1d/97/87bb66ff62fb1a9a6d216e31913778335a125be813597f3a70d88b077e0f/pypinyin-0.44.0-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kaldiio (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5b/64/a615d1f8b31ee3b3eee9b1e3e17b9961757c0c1fb7df5b7dbd9baef004a9/kaldiio-2.17.2.tar.gz\n",
      "Requirement already satisfied: soundfile~=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.10.3.post1)\n",
      "Collecting GPUtil (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
      "Collecting pyworld (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/88/0b/9f8ceb7548bbb1294729b291044b8e72754db21dbc672acbd5eec6ed740b/pyworld-0.3.0.tar.gz (212kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting coverage (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/78/e7f9f4e28c237f3909ed184939cae5bf19e6507459185286afcacb730578/coverage-6.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting g2pM (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/af/21/dc5b497f09a94a9605e0b8a94ad0e01ae73a2b65109bf5bd325b0814b6a8/g2pM-0.1.2.5-py3-none-any.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.9.0)\n",
      "Requirement already satisfied: llvmlite in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.31.0)\n",
      "Collecting sacrebleu (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/63/b3c11f951eafa2dc296862431f29fb12dbe191cb72217cf88ed04c32086b/sacrebleu-2.0.0-py3-none-any.whl (90kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 3.3MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting distro (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/8d/a0a5c389d76f90c766e956515d34c3408a1e18f60fbaa08221d1f6b87490/distro-1.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.8.0)\n",
      "Collecting ConfigArgParse (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/af/cb/2a6620656f029b7b49c302853b433fac2c8eda9cbb5a3bc70b186b1b5b90/ConfigArgParse-1.5.3-py3-none-any.whl\n",
      "Collecting webrtcvad (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/34/e2de2d97f3288512b9ea56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.9MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting yacs (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
      "Collecting gpustat (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pypi-kenlm (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9c/cb/67310dc4524d61ed6460d7618709b40e81b82922fdbd9cb78a6e50ec6d86/pypi-kenlm-0.1.20210121.tar.gz (253kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.2.3)\n",
      "Collecting nara-wpe (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/d4/11dddfd5f41017df8eda83cbcafab14ba8bf32d23e7697bf9d2bd343d979/nara_wpe-0.0.7-py3-none-any.whl\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.8)\n",
      "Collecting inflect (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4f/a8/031641ad73a1bd1a9932261a6193864556172b333dde263fed8b5a0940cf/inflect-5.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.2.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (3.4.5)\n",
      "Collecting paddlespeech-feat (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c0/87/d0bc9cce473630ae11747be46dae3d1da7de14a35991bcec1ac058eab4d5/paddlespeech_feat-0.0.1a0-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.1.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (7.1.2)\n",
      "Collecting soxbindings (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/08/57/571daf4671e5e3e4f0d6a6ad8a45e66f92f0a3bfe8d0417d24eefa6b136f/soxbindings-1.2.3-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (4.27.0)\n",
      "Collecting snakeviz (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/78/38b6e9ec9a17fca63c7f83eb88e8e368a93b771be9168142c34e4c42f5ef/snakeviz-2.1.1-py2.py3-none-any.whl (282kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pynvml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (8.0.4)\n",
      "Collecting g2p-en (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d7/d9/b77dc634a7a0c0c97716ba97dd0a28cbfa6267c96f359c4f27ed71cbd284/g2p_en-2.1.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting unidecode (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/3a/3c35e04ea05724f29c77bf5c7a27cf0c80310483655bfdc2c1c5d1ab36b6/Unidecode-1.3.2-py3-none-any.whl (235kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 6.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yq (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/97/0a/b5a11f52ac794fbf800e05b6268283e6aea412a3faa5ae95e67eb29ea5e0/yq-2.12.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.48.0)\n",
      "Requirement already satisfied: librosa in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.7.2)\n",
      "Requirement already satisfied: resampy==0.2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.2.2)\n",
      "Collecting sox (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3f/67/1810e9a69956eb236967b7174c11fd8d8c2cdab051509286f72e6c7e147e/sox-1.4.1-py2.py3-none-any.whl\n",
      "Collecting timer (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/c2/9b06bd96cc1041f3aa0bb07acdbfddbefd8c0ca300fc9ad696afb77140d1/timer-0.2.2-py3-none-any.whl\n",
      "Collecting paddlespeech-ctcdecoders (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/72/b47065fe1a9ec1133625c1446787291a9cee567657b558320507dca08998/paddlespeech_ctcdecoders-0.0.2a0-cp37-cp37m-manylinux1_x86_64.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pybind11 (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a8/3b/fc246e1d4c7547a7a07df830128e93c6215e9b93dcb118b2a47a70726153/pybind11-2.8.1-py2.py3-none-any.whl (208kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard (from paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/23/08b5774e60ad676a898cf27dc18744a5d73a5f93db6e8ce0ea2d908dca59/typeguard-2.13.2-py3-none-any.whl\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.42.1)\n",
      "Requirement already satisfied: sentencepiece~=0.1.96 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.1.96)\n",
      "Collecting phkit (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/7a/eadf1985f29daff6950d2b04e8b87d454404e239b9386b4c505c3b7bae31/phkit-0.2.10-py3-none-any.whl (800kB)\n",
      "\u001b[K     |████████████████████████████████| 808kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting loguru (from paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 264kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.3.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.15.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.4.10)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (2.0.1)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (16.7.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (0.23)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (5.1.2)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (0.10.0)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scipy->paddlespeech) (1.20.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from soundfile~=0.10->paddlespeech) (1.14.0)\n",
      "Requirement already satisfied: cython>=0.24.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyworld->paddlespeech) (0.29)\n",
      "Collecting regex (from sacrebleu->paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7f/e3/5bd531ce6dc4c08b41173576a377fff5e6e6ca87bb58d926acb2218e2b44/regex-2021.11.10-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (670kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 4.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacrebleu->paddlespeech) (0.4.4)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu->paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ca/80/7c0cad11bd99985cfe7c09427ee0b4f9bd6b048bd13d4ffb32c6db237dfb/tabulate-0.8.9-py3-none-any.whl\n",
      "Collecting portalocker (from sacrebleu->paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/63/eb/f84872af6e9312ea2f345b218015a41191cfd37eeba4a4fd228f241c2a75/portalocker-2.3.2-py2.py3-none-any.whl\n",
      "Collecting nvidia-ml-py3>=7.352.0 (from gpustat->paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/64/cce82bddb80c0b0f5c703bbdafa94bfb69a1c5ad7a79cff00b482468f0d3/nvidia-ml-py3-7.352.0.tar.gz\n",
      "Collecting blessings>=1.6 (from gpustat->paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (2.4.2)\n",
      "Collecting bottleneck (from nara-wpe->paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5b/08/278c6ee569458e168096f6b51019cc1c81c288da3d1026a22ee2ccead102/Bottleneck-1.3.2.tar.gz (88kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 139kB/s eta 0:00:011\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from nara-wpe->paddlespeech) (7.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboardX->paddlespeech) (3.14.0)\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (0.7.1.1)\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (3.8.2)\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (1.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (2.22.0)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (1.0.0)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (0.8.53)\n",
      "Collecting mock (from paddlespeech-feat->paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: tornado>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from snakeviz->paddlespeech) (6.1)\n",
      "Collecting distance>=0.1.3 (from g2p-en->paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from yq->paddlespeech) (56.2.0)\n",
      "Collecting xmltodict>=0.11.0 (from yq->paddlespeech)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: argcomplete>=1.8.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from yq->paddlespeech) (1.12.3)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (2.1.8)\n",
      "Requirement already satisfied: joblib>=0.12 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (0.24.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (4.4.2)\n",
      "Collecting hanziconv (from phkit->paddlespeech)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/63/71/b89cb63077fd807fe31cf7c016a06e7e579a289d8a37aa24a30282d02dd2/hanziconv-0.3.2.tar.gz (276kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->paddlespeech) (3.6.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from cffi>=1.0->soundfile~=0.10->paddlespeech) (2.19)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlespeech) (0.6.1)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlespeech) (2.2.0)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlespeech) (2.6.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlespeech) (0.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlespeech) (2.11.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlespeech) (1.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (2019.9.11)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlespeech) (2.8.0)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlespeech) (0.18.0)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlespeech) (3.9.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->paddlespeech) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlespeech) (1.1.1)\n",
      "Building wheels for collected packages: pyworld, bottleneck\n",
      "  Building wheel for pyworld (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyworld: filename=pyworld-0.3.0-cp37-cp37m-linux_x86_64.whl size=665050 sha256=f43daa4b322b9a03d150e15920fb515e1cc02d465b07a380e35535d483f36eb7\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/76/67/87/cd5b7cf3752fae49bff502159461540eb607537c8744b4a7f1\n",
      "  Building wheel for bottleneck (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bottleneck: filename=Bottleneck-1.3.2-cp37-cp37m-linux_x86_64.whl size=335227 sha256=282f8f144d9153a87323bbed6cc7c221c4ac57252e35c593b9e6d02eb723cbb0\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/29/37/51/e857dfd6fae1ed45ad984f0e687e44d7357f5abb48171fdd66\n",
      "Successfully built pyworld bottleneck\n",
      "Building wheels for collected packages: kaldiio, GPUtil, webrtcvad, gpustat, pypi-kenlm, nvidia-ml-py3, distance, hanziconv\n",
      "  Building wheel for kaldiio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaldiio: filename=kaldiio-2.17.2-cp37-none-any.whl size=24470 sha256=df3ac4dc65f9506d9f923b22765da0b2a0da093fb1ffc514559d9d8df3c32bd3\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/90/56/66/168d57ad173090bce491569855ea29ac152b9ef11432f97a0a\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=0526e6b9af8a58d801e03ebd802573d17e02e61fe95d52cbeb3e435d4cb3a402\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/fc/9a/9d/4a1e1b88304ad876abfdc19dbdf1b538424b0071157a941456\n",
      "  Building wheel for webrtcvad (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp37-cp37m-linux_x86_64.whl size=92988 sha256=d12c39e7bd0097f9ff2f7df4e1ca7770f9cba741ca17aae521628ad580048e2e\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/50/c3/3f/eaa838f8a45646e43d88dce3f888b5b49e5c76aae490355382\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=9f6d0990c6d2e5902b9ff54856bf951857333d6ad3dd05c8c813455387d5e031\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/f3/b5/ec/3dc6dc87612885063182a103de99f497e3699e5df1c6712d28\n",
      "  Building wheel for pypi-kenlm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypi-kenlm: filename=pypi_kenlm-0.1.20210121-cp37-cp37m-linux_x86_64.whl size=2309695 sha256=1e9541e745224c61453e6745d1025a1f9cfa56107b0f081fe4a99f68370e88fe\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/2c/5d/70/fca740a1c46ac4ef08db56a086ffdd821c85a39d5155605188\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-cp37-none-any.whl size=19193 sha256=a0e110b2b931da95c1f183e8795855a6a5970b26687523cc131cacfdc84939a7\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/6b/46/ea/a381e40570a4ba4836ddf59fb46d9bc5d91d8bdcac1050e542\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-cp37-none-any.whl size=16261 sha256=0025cc26cf6f7e3eb27ab595c5967fbe8059ca04f34f5d05ab6ccd215bf71d4a\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/38/a6/ef/319bf90d5feb5d9efabfa7bec9d427a7960c64335140fdd4fc\n",
      "  Building wheel for hanziconv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hanziconv: filename=hanziconv-0.3.2-py2.py3-none-any.whl size=23215 sha256=a6833325a75186b4b5cce8f78ebe34867c276bb9575df39c07c6aa12901ddf95\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/53/07/bd/49ef28c478be1163599d23e1c948d6cafda09e7cc6df905261\n",
      "Successfully built kaldiio GPUtil webrtcvad gpustat pypi-kenlm nvidia-ml-py3 distance hanziconv\n",
      "\u001b[31mERROR: blackhole 1.0.1 has requirement numpy<=1.19.5, but you'll have numpy 1.20.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: blackhole 1.0.1 has requirement tabulate==0.8.3, but you'll have tabulate 0.8.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: textgrid, praatio, jsonlines, editdistance, pypinyin, kaldiio, GPUtil, pyworld, coverage, g2pM, regex, tabulate, portalocker, sacrebleu, distro, ConfigArgParse, webrtcvad, yacs, nvidia-ml-py3, blessings, gpustat, pypi-kenlm, bottleneck, nara-wpe, inflect, mock, paddlespeech-feat, sox, soxbindings, snakeviz, distance, g2p-en, unidecode, xmltodict, yq, timer, paddlespeech-ctcdecoders, pybind11, typeguard, hanziconv, phkit, loguru, paddlespeech\n",
      "  Found existing installation: tabulate 0.8.3\n",
      "    Uninstalling tabulate-0.8.3:\n",
      "      Successfully uninstalled tabulate-0.8.3\n",
      "Successfully installed ConfigArgParse-1.5.3 GPUtil-1.4.0 blessings-1.7 bottleneck-1.3.2 coverage-6.2 distance-0.1.3 distro-1.6.0 editdistance-0.6.0 g2p-en-2.1.0 g2pM-0.1.2.5 gpustat-0.6.0 hanziconv-0.3.2 inflect-5.3.0 jsonlines-2.0.0 kaldiio-2.17.2 loguru-0.5.3 mock-4.0.3 nara-wpe-0.0.7 nvidia-ml-py3-7.352.0 paddlespeech-0.1.0a1 paddlespeech-ctcdecoders-0.0.2a0 paddlespeech-feat-0.0.1a0 phkit-0.2.10 portalocker-2.3.2 praatio-4.4.0 pybind11-2.8.1 pypi-kenlm-0.1.20210121 pypinyin-0.44.0 pyworld-0.3.0 regex-2021.11.10 sacrebleu-2.0.0 snakeviz-2.1.1 sox-1.4.1 soxbindings-1.2.3 tabulate-0.8.9 textgrid-1.5 timer-0.2.2 typeguard-2.13.2 unidecode-1.3.2 webrtcvad-2.0.10 xmltodict-0.12.0 yacs-0.1.8 yq-2.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlespeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 准备工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/work\n",
      "/home/aistudio/work/workspace_asr\n"
     ]
    }
   ],
   "source": [
    "%cd ./work\n",
    "!mkdir -p ./workspace_asr\n",
    "%cd ./workspace_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 获取预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-30 11:22:18--  https://paddlespeech.bj.bcebos.com/s2t/aishell/asr1/transformer.model.tar.gz\n",
      "Resolving paddlespeech.bj.bcebos.com (paddlespeech.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c04:1001:1002:0:ff:b001:368a\n",
      "Connecting to paddlespeech.bj.bcebos.com (paddlespeech.bj.bcebos.com)|182.61.200.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 123699838 (118M) [application/octet-stream]\n",
      "Saving to: ‘transformer.model.tar.gz’\n",
      "\n",
      "transformer.model.t 100%[===================>] 117.97M  47.3MB/s    in 2.5s    \n",
      "\n",
      "2021-11-30 11:22:20 (47.3 MB/s) - ‘transformer.model.tar.gz’ saved [123699838/123699838]\n",
      "\n",
      "conf/transformer.yaml\n",
      "conf/preprocess.yaml\n",
      "data/mean_std.json\n",
      "exp/transformer/checkpoints/avg_20.pdparams\n",
      "data/lang_char/\n",
      "data/lang_char/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://paddlespeech.bj.bcebos.com/s2t/aishell/asr1/transformer.model.tar.gz\n",
    "!tar xzvf transformer.model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取用于预测的音频文件\n",
    "%cp ../data/BAC009S0908W0355.wav ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 导入python包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from paddlespeech.s2t.exps.u2.config import get_cfg_defaults\n",
    "from paddlespeech.s2t.frontend.featurizer.text_featurizer import TextFeaturizer\n",
    "#from paddlespeech.s2t.io.collator import SpeechCollator\n",
    "from paddlespeech.s2t.models.u2 import U2Model\n",
    "from paddlespeech.s2t.utils import layer_tools\n",
    "\n",
    "#from paddlespeech.s2t.frontend.normalizer import FeatureNormalizer\n",
    "#from paddlespeech.s2t.frontend.featurizer.audio_featurizer import AudioFeaturizer\n",
    "#from paddlespeech.s2t.frontend.speech import SpeechSegment\n",
    "\n",
    "from paddlespeech.s2t.transform.spectrogram import LogMelSpectrogramKaldi\n",
    "from paddlespeech.s2t.transform.cmvn import GlobalCMVN\n",
    "import soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 设置预训练模型的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Config========\n",
      "collator:\n",
      "  augmentation_config: conf/preprocess.yaml\n",
      "  batch_size: 64\n",
      "  delta_delta: False\n",
      "  dither: 1.0\n",
      "  feat_dim: 80\n",
      "  keep_transcription_text: False\n",
      "  max_freq: None\n",
      "  mean_std_filepath: \n",
      "  n_fft: None\n",
      "  num_workers: 2\n",
      "  random_seed: 0\n",
      "  raw_wav: True\n",
      "  shuffle_method: batch_shuffle\n",
      "  sortagrad: True\n",
      "  spectrum_type: fbank\n",
      "  spm_model_prefix: \n",
      "  stride_ms: 10.0\n",
      "  target_dB: -20\n",
      "  target_sample_rate: 16000\n",
      "  unit_type: char\n",
      "  use_dB_normalization: True\n",
      "  vocab_filepath: data/lang_char/vocab.txt\n",
      "  window_ms: 25.0\n",
      "data:\n",
      "  dev_manifest: data/manifest.dev\n",
      "  manifest: \n",
      "  max_input_len: 20.0\n",
      "  max_output_input_ratio: 10.0\n",
      "  max_output_len: 400.0\n",
      "  min_input_len: 0.5\n",
      "  min_output_input_ratio: 0.05\n",
      "  min_output_len: 0.0\n",
      "  test_manifest: data/manifest.test\n",
      "  train_manifest: data/manifest.train\n",
      "decoding:\n",
      "  alpha: 2.5\n",
      "  batch_size: 128\n",
      "  beam_size: 10\n",
      "  beta: 0.3\n",
      "  ctc_weight: 0.5\n",
      "  cutoff_prob: 1.0\n",
      "  cutoff_top_n: 0\n",
      "  decoding_chunk_size: -1\n",
      "  decoding_method: attention\n",
      "  error_rate_type: cer\n",
      "  lang_model_path: data/lm/common_crawl_00.prune01111.trie.klm\n",
      "  num_decoding_left_chunks: -1\n",
      "  num_proc_bsearch: 8\n",
      "  simulate_streaming: False\n",
      "model:\n",
      "  cmvn_file: None\n",
      "  cmvn_file_type: json\n",
      "  decoder: transformer\n",
      "  decoder_conf:\n",
      "    attention_heads: 4\n",
      "    dropout_rate: 0.1\n",
      "    linear_units: 2048\n",
      "    num_blocks: 6\n",
      "    positional_dropout_rate: 0.1\n",
      "    self_attention_dropout_rate: 0.0\n",
      "    src_attention_dropout_rate: 0.0\n",
      "  encoder: transformer\n",
      "  encoder_conf:\n",
      "    attention_dropout_rate: 0.0\n",
      "    attention_heads: 4\n",
      "    dropout_rate: 0.1\n",
      "    input_layer: conv2d\n",
      "    linear_units: 2048\n",
      "    normalize_before: True\n",
      "    num_blocks: 12\n",
      "    output_size: 256\n",
      "    positional_dropout_rate: 0.1\n",
      "  input_dim: 0\n",
      "  model_conf:\n",
      "    ctc_dropoutrate: 0.0\n",
      "    ctc_grad_norm_type: None\n",
      "    ctc_weight: 0.3\n",
      "    length_normalized_loss: False\n",
      "    lsm_weight: 0.1\n",
      "  output_dim: 0\n",
      "training:\n",
      "  accum_grad: 2\n",
      "  checkpoint:\n",
      "    kbest_n: 50\n",
      "    latest_n: 5\n",
      "  global_grad_clip: 5.0\n",
      "  log_interval: 100\n",
      "  n_epoch: 120\n",
      "  optim: adam\n",
      "  optim_conf:\n",
      "    lr: 0.002\n",
      "    weight_decay: 1e-06\n",
      "  scheduler: warmuplr\n",
      "  scheduler_conf:\n",
      "    lr_decay: 1.0\n",
      "    warmup_steps: 25000\n"
     ]
    }
   ],
   "source": [
    "config_path = \"conf/transformer.yaml\" \n",
    "checkpoint_path = \"./exp/transformer/checkpoints/avg_20.pdparams\"\n",
    "decoding_method = \"attention\"\n",
    "audio_file = \"data/BAC009S0908W0355.wav\"\n",
    "\n",
    "result_file = \"exp/result.rsl\"\n",
    "\n",
    "# 读取 conf 文件并结构化\n",
    "transformer_config = get_cfg_defaults()\n",
    "transformer_config.merge_from_file(config_path)\n",
    "transformer_config.decoding.decoding_method = decoding_method\n",
    "\n",
    "#transformer_config = CfgNode(new_allowed=True)\n",
    "#transformer_config.merge_from_file(config_path)\n",
    "print(\"========Config========\")\n",
    "print(transformer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage 1 获取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 音频特征Fbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![信号处理流水线](work/source/signal_pipeline.png)\n",
    "(摘自https://github.com/PaddlePaddle/PaddleSpeech/blob/develop/docs/tutorial/tts/tts_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建音频特征提取对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建 logmel 特征\n",
    "logmel_kaldi= LogMelSpectrogramKaldi(\n",
    "            fs= 16000,\n",
    "            n_mels= 80,\n",
    "            n_shift= 160,\n",
    "            win_length= 400,\n",
    "            dither= True)\n",
    "\n",
    "# 特征减均值除以方差\n",
    "cmvn = GlobalCMVN(\n",
    "    cmvn_path=\"data/mean_std.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 提取音频的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Feature========\n",
      "Tensor(shape=[848, 80], dtype=float32, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[ 0.51146847,  0.25355875, -1.69958019, ..., -0.66561252,\n",
      "         -0.69228119, -0.72872376],\n",
      "        [ 0.06364148, -0.52314031, -0.86850190, ..., -1.15806091,\n",
      "         -0.92412323, -0.80306697],\n",
      "        [-0.17580508, -0.36929476, -1.90482414, ..., -0.95011121,\n",
      "         -0.91865319, -0.78732079],\n",
      "        ...,\n",
      "        [ 1.01879728,  0.65635836, -0.79088914, ..., -0.98369741,\n",
      "         -1.01682007, -0.99973106],\n",
      "        [-0.49923953, -0.53835851, -0.24169487, ..., -1.13909304,\n",
      "         -1.07762146, -0.82446492],\n",
      "        [ 0.13295671,  0.08121970, -0.92406386, ..., -1.22621000,\n",
      "         -1.33519983, -1.00139403]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1130 11:22:27.251442 32585 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W1130 11:22:27.256433 32585 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "array, _ = soundfile.read(audio_file, dtype=\"int16\")\n",
    "array = logmel_kaldi(array, train=False)\n",
    "array = cmvn(array)\n",
    "audio_feature = array\n",
    "\n",
    "print(\"========Feature========\")\n",
    "\n",
    "audio_len = audio_feature.shape[0]\n",
    "audio_feature = paddle.to_tensor(audio_feature, dtype='float32')\n",
    "print (audio_feature)\n",
    "audio_len = paddle.to_tensor(audio_len)\n",
    "audio_feature = paddle.unsqueeze(audio_feature, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage 2 声学模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Transofomer 语音识别模型的结构\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"work/source/transformer.png\"/>\n",
    "</div>\n",
    "\n",
    "图片参考了https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "\n",
    "Transformer模型主要由2个部分组成，包括transformer encoder和transformer decoder。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Transformer Encoder\n",
    "\n",
    "Transformer encoder主要是对音频的原始特征（这里原始特征使用的是80维fbank）进行特征编码，其输入是fbank，输出是特征编码。一个Transformer Encoder由一个结合了位置编码(position encoding)的降采样模块(subsampling embedding)和多个Transformer Encoder Layer组成。  \n",
    "\n",
    "其中降采样模块一般由2层降采样的CNN构成。而一层Transformer Encoder Layer主要由 Multi-head attention和Feed forward Layer构成。这里使用的Multi-head attention使用了self-attention方式，主要特点是Q(query), K(key)和V(value)都是同一个输入。而Feed forward layer由两层全连接层构建，其特点是保持了输入和输出的特征维度是一致的。 另外，encoder采用了残差网络的结构，并分别应用在了Multi-head attention 和 Feed forward Layer两个模块上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Multi-Head Attention 机制\n",
    "<div align=center>\n",
    "<img src=\"work/source/Attention.png\" />\n",
    "</div>\n",
    "> 图片参考了https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "\n",
    "对于self-attention的方式，由于其Q，K，V都是相同的，因此可以用如下的示例图更加清晰地表示：\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"work/source/Attention_detail.png\" />\n",
    "</div>\n",
    "\n",
    "其主要步骤可以分为三步：\n",
    "\n",
    "第一步：\n",
    "Q和K的向量通过求内积的方式计算相似度，经过scale和softmax后，获得每个Q和所有K之间的score。\n",
    "\n",
    "第二步：\n",
    "将每个Q和所有K之间的score和V进行相乘，再将相乘后的结果想加，得到attetion的输出向量。\n",
    "\n",
    "第三步：\n",
    "用多个Attetion模块都进行第一步和第二步，并将最后的输出向量进行合并，得到最后的Multi-Head Attention输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Transformer的Encoder构建代码\n",
    "\n",
    "transformer Encoder主要由多层的 transformer encoder layer组成\n",
    "\n",
    "```python\n",
    "\"\"\" 构建 TransformerEncoder\n",
    "        \n",
    "\"\"\"\n",
    "class TransformerEncoder(BaseEncoder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            output_size: int=256,\n",
    "            attention_heads: int=4,\n",
    "            linear_units: int=2048,\n",
    "            num_blocks: int=6,\n",
    "            dropout_rate: float=0.1,\n",
    "            positional_dropout_rate: float=0.1,\n",
    "            attention_dropout_rate: float=0.0,\n",
    "            input_layer: str=\"conv2d\",\n",
    "            pos_enc_layer_type: str=\"abs_pos\",\n",
    "            normalize_before: bool=True,\n",
    "            concat_after: bool=False,\n",
    "            static_chunk_size: int=0,\n",
    "            use_dynamic_chunk: bool=False,\n",
    "            global_cmvn: nn.Layer=None,\n",
    "            use_dynamic_left_chunk: bool=False, ):\n",
    "        \n",
    "        assert check_argument_types()\n",
    "        super().__init__(input_size, output_size, attention_heads, linear_units,\n",
    "                         num_blocks, dropout_rate, positional_dropout_rate,\n",
    "                         attention_dropout_rate, input_layer,\n",
    "                         pos_enc_layer_type, normalize_before, concat_after,\n",
    "                         static_chunk_size, use_dynamic_chunk, global_cmvn,\n",
    "                         use_dynamic_left_chunk)\n",
    "        self.encoders = nn.LayerList([\n",
    "            TransformerEncoderLayer(\n",
    "                size=output_size,\n",
    "                self_attn=MultiHeadedAttention(attention_heads, output_size,\n",
    "                                               attention_dropout_rate),\n",
    "                feed_forward=PositionwiseFeedForward(output_size, linear_units,\n",
    "                                                     dropout_rate),\n",
    "                dropout_rate=dropout_rate,\n",
    "                normalize_before=normalize_before,\n",
    "                concat_after=concat_after) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            xs: paddle.Tensor,\n",
    "            xs_lens: paddle.Tensor,\n",
    "            decoding_chunk_size: int=0,\n",
    "            num_decoding_left_chunks: int=-1,\n",
    "    ) -> Tuple[paddle.Tensor, paddle.Tensor]:\n",
    "        \"\"\"Embed positions in tensor.\n",
    "        Args:\n",
    "            xs: 经过了padding的输入  (B, L, D)\n",
    "            xs_lens: 输入长度 (B)\n",
    "            decoding_chunk_size: 解码的chunk的动态长度\n",
    "                0: 用于训练, 使用动态素鸡的chunk长度.\n",
    "                <0: for decoding, 使用整句话.\n",
    "                >0: for decoding, 输入decoding_chunk_size的长度.\n",
    "            num_decoding_left_chunks: 使用已经解码的chunk数进行decoding,\n",
    "                >=0: 使用num_decoding_left_chunks个\n",
    "                <0: 使用所有的chunk\n",
    "        Returns:\n",
    "            xs： Encoder输出tensor\n",
    "            masks: Encoder输出tensor的mask\n",
    "        \"\"\"\n",
    "        masks = make_non_pad_mask(xs_lens).unsqueeze(1)  # (B, 1, L)\n",
    "\n",
    "        if self.global_cmvn is not None:\n",
    "            xs = self.global_cmvn(xs)\n",
    "        xs, pos_emb, masks = self.embed(xs, masks.astype(xs.dtype), offset=0)\n",
    "        masks = masks.astype(paddle.bool)\n",
    "        mask_pad = masks.logical_not()\n",
    "        chunk_masks = add_optional_chunk_mask(\n",
    "            xs, masks, self.use_dynamic_chunk, self.use_dynamic_left_chunk,\n",
    "            decoding_chunk_size, self.static_chunk_size,\n",
    "            num_decoding_left_chunks)\n",
    "        for layer in self.encoders:\n",
    "            xs, chunk_masks, _ = layer(xs, chunk_masks, pos_emb, mask_pad)\n",
    "        if self.normalize_before:\n",
    "            xs = self.after_norm(xs)\n",
    "            \n",
    "        # 这里我们假设Encoder中的掩码没有改变，所以只是\n",
    "        # 返回Encoder输入的mask，这些mask将被使用于Decoder中\n",
    "        return xs, masks\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "class TransformerEncoderLayer(nn.Layer):\n",
    "    \"\"\"Encoder layer module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            size: int,\n",
    "            self_attn: nn.Layer,\n",
    "            feed_forward: nn.Layer,\n",
    "            dropout_rate: float,\n",
    "            normalize_before: bool=True,\n",
    "            concat_after: bool=False, ):\n",
    "        \"\"\"构建Encoder 层.\n",
    "        Args:\n",
    "            size (int): Input dimension.\n",
    "            self_attn (nn.Layer): 自注意力模块实例.\n",
    "                `MultiHeadedAttention` 或者 `RelPositionMultiHeadedAttention`的实例\n",
    "            feed_forward (nn.Layer): Feed-forward 模块实例.\n",
    "      \t\t\t\t使用`PositionwiseFeedForward`的实例\n",
    "            dropout_rate (float): Dropout rate.\n",
    "            normalize_before (bool):\n",
    "                True: 在 sub-block前使用 layer-norm.\n",
    "                False: 在 sub-block后使用 layer-norm.\n",
    "            concat_after (bool): 是否合并attention层的输入和输出\n",
    "                True: x -> x + linear(concat(x, att(x)))\n",
    "                False: x -> x + att(x)\n",
    "\t\t\"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.norm1 = nn.LayerNorm(size, epsilon=1e-12)\n",
    "        self.norm2 = nn.LayerNorm(size, epsilon=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.size = size\n",
    "        self.normalize_before = normalize_before\n",
    "        self.concat_after = concat_after\n",
    "        # concat_linear 一般情况下不会使用到，但会在模型保存的时候存储下来\n",
    "        self.concat_linear = nn.Linear(size + size, size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: paddle.Tensor,\n",
    "            mask: paddle.Tensor,\n",
    "            pos_emb: Optional[paddle.Tensor]=None,\n",
    "            mask_pad: Optional[paddle.Tensor]=None,\n",
    "            output_cache: Optional[paddle.Tensor]=None,\n",
    "            cnn_cache: Optional[paddle.Tensor]=None,\n",
    "    ) -> Tuple[paddle.Tensor, paddle.Tensor, paddle.Tensor]:\n",
    "        \"\"\"Compute encoded features.\n",
    "        Args:\n",
    "            x (paddle.Tensor): 输入 Tensor (#batch, time, size).\n",
    "            mask (paddle.Tensor): 输入的 Mask tensor (#batch, time).\n",
    "            pos_emb (paddle.Tensor): 位置编码， 这里只是为了和ConformerEncoderLayer保持接口兼容性\n",
    "            mask_pad (paddle.Tensor): 没有使用这个参数，这里只是为了和ConformerEncoderLayer保持接口兼容性\n",
    "            output_cache (paddle.Tensor): 输出的缓存  (#batch, time2, size), time2 < time in x.\n",
    "            cnn_cache (paddle.Tensor): 没有使用这个参数，这里只是为了和ConformerEncoderLayer保持接口兼容性\n",
    "        Returns:\n",
    "            x: paddle.Tensor: 输出 tensor (#batch, time, size).\n",
    "            mask: paddle.Tensor: mask tensor (#batch, time).\n",
    "            fake_cnn_cache: paddle.Tensor: ，这里只是为了和Conformer保持接口兼容性 (#batch, channels, time').\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        if output_cache is None:\n",
    "            x_q = x\n",
    "        else:\n",
    "            assert output_cache.shape[0] == x.shape[0]\n",
    "            assert output_cache.shape[1] < x.shape[1]\n",
    "            assert output_cache.shape[2] == self.size\n",
    "            chunk = x.shape[1] - output_cache.shape[1]\n",
    "            x_q = x[:, -chunk:, :]\n",
    "            residual = residual[:, -chunk:, :]\n",
    "            mask = mask[:, -chunk:, :]\n",
    "\n",
    "        if self.concat_after:\n",
    "            x_concat = paddle.concat(\n",
    "                (x, self.self_attn(x_q, x, x, mask)), axis=-1)\n",
    "            x = residual + self.concat_linear(x_concat)\n",
    "        else:\n",
    "            x = residual + self.dropout(self.self_attn(x_q, x, x, mask))\n",
    "        if not self.normalize_before:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.norm2(x)\n",
    "        x = residual + self.dropout(self.feed_forward(x))\n",
    "        if not self.normalize_before:\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        if output_cache is not None:\n",
    "            x = paddle.concat([output_cache, x], axis=1)\n",
    "\n",
    "        fake_cnn_cache = paddle.zeros([1], dtype=x.dtype)\n",
    "        return x, mask, fake_cnn_cache\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Transformer Decoder\n",
    "\n",
    "Transformer的Decoder用于获取最后输出的结果。其结构和Encoder有一定的相似性，也具有Multi-head attention模块和Feed forward layer。主要的不同点有2个，第一个不同点是Decoder采用的是一种自回归的方式进行解码。而第二个是在于Decoder在Multi-head attention和Feed forward layer模块之间增加了一层Multi-head attention层用于获取Encoder得到的特征编码。\n",
    "\n",
    "\n",
    "\n",
    "#### Decoder的自回归解码 \n",
    "其采用了一种自回归的结构，即decoder的上一个时间点的输出会作为下一个时间点的输入。另外，计算的过程中，decoder会利用encoder的输出信息。如果使用greedy的方式，decoder的解码过程如下：\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"work/source/attentiondecode_process_greedy.png\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "使用greedy模式解码比较简单，但是很有可能会在解码过程中丢失整体上效果更好的解码结果，因此我们实际使用的是beam search方式的解码，beam search模式下的decoder的解码过程如下：\n",
    "\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"work/source/attentiondecode_process_greedy.png\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Decoder获取Encoder的K和V进行attention\n",
    "<div align=center>\n",
    "<img src=\"work/source/src_attention.png\"  />\n",
    "</div>\n",
    "\n",
    "Decoder在每一步的解码过程中，都会利用Encoder的输出的特征编码进行Multi-head attention。\n",
    "\n",
    "其中Decoder会将对自回结果的编码作为attention中的Q，而Encoder输出的特征编码作为K和V来完成attetion计算，从而利用Encoder提取的音频信息。\n",
    "\n",
    "\n",
    "\n",
    "#### （细节）Masked Multi-head Attention\n",
    "细心的通许可能发现了，Decoder的一个multi-head attention前头有一个mask。增加了这个mask的原因在于进行Decoder训练的时候，Decoder的输入是一句完整的句子，而不是像预测这样一步步输入句子的前缀。为了模拟预测的过程，Decoder训练的时候需要用mask遮住句子。例如T=1的时候，就要mask住输入中除第一个字符以外其他的字符，T=2的时候则是遮住除前两个字符以外的其余字符。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Transformer的Decoder构建代码\n",
    "```python\n",
    "class TransformerDecoder(BatchScorerInterface, nn.Layer):\n",
    "    \"\"\"Base class of Transfomer decoder module.\n",
    "    Args:\n",
    "        vocab_size: 输出维数\n",
    "        encoder_output_size: 等价于attention的输出维数\n",
    "        attention_heads: multi head attention中的head的数目\n",
    "        linear_units: position-wise feedforward中hidden层的维数\n",
    "        num_blocks: decoder blocks的数目\n",
    "        dropout_rate: dropout rate\n",
    "        self_attention_dropout_rate: attention的dropout rate\n",
    "        input_layer: 输入层的类型，例如'embed'\n",
    "        use_output_layer: 是否使用output layer\n",
    "        pos_enc_class: PositionalEncoding的类\n",
    "        normalize_before:\n",
    "             True: 在 sub-block前使用 layer-norm.\n",
    "             False: 在 sub-block后使用 layer-norm.\n",
    "        concat_after (bool): 是否合并attention层的输入和输出\n",
    "                True: x -> x + linear(concat(x, att(x)))\n",
    "                False: x -> x + att(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            encoder_output_size: int,\n",
    "            attention_heads: int=4,\n",
    "            linear_units: int=2048,\n",
    "            num_blocks: int=6,\n",
    "            dropout_rate: float=0.1,\n",
    "            positional_dropout_rate: float=0.1,\n",
    "            self_attention_dropout_rate: float=0.0,\n",
    "            src_attention_dropout_rate: float=0.0,\n",
    "            input_layer: str=\"embed\",\n",
    "            use_output_layer: bool=True,\n",
    "            normalize_before: bool=True,\n",
    "            concat_after: bool=False, ):\n",
    "\n",
    "        assert check_argument_types()\n",
    "        nn.Layer.__init__(self)\n",
    "        self.selfattention_layer_type = 'selfattn'\n",
    "        attention_dim = encoder_output_size\n",
    "\n",
    "        if input_layer == \"embed\":\n",
    "            self.embed = nn.Sequential(\n",
    "                nn.Embedding(vocab_size, attention_dim),\n",
    "                PositionalEncoding(attention_dim, positional_dropout_rate), )\n",
    "        else:\n",
    "            raise ValueError(f\"only 'embed' is supported: {input_layer}\")\n",
    "\n",
    "        self.normalize_before = normalize_before\n",
    "        self.after_norm = nn.LayerNorm(attention_dim, epsilon=1e-12)\n",
    "        self.use_output_layer = use_output_layer\n",
    "        self.output_layer = nn.Linear(attention_dim, vocab_size)\n",
    "\n",
    "        self.decoders = nn.LayerList([\n",
    "            DecoderLayer(\n",
    "                size=attention_dim,\n",
    "                self_attn=MultiHeadedAttention(attention_heads, attention_dim,\n",
    "                                               self_attention_dropout_rate),\n",
    "                src_attn=MultiHeadedAttention(attention_heads, attention_dim,\n",
    "                                              src_attention_dropout_rate),\n",
    "                feed_forward=PositionwiseFeedForward(\n",
    "                    attention_dim, linear_units, dropout_rate),\n",
    "                dropout_rate=dropout_rate,\n",
    "                normalize_before=normalize_before,\n",
    "                concat_after=concat_after, ) for _ in range(num_blocks)\n",
    "        ])\n",
    "```\n",
    "\n",
    "```python\n",
    "class DecoderLayer(nn.Layer):\n",
    "    \"\"\"Single decoder layer module.\n",
    "    Args:\n",
    "        size (int): Input 的维度数.\n",
    "        self_attn (nn.Layer): 自注意力模块实例.\n",
    "            `MultiHeadedAttention` 的实例可以作为参数.\n",
    "        src_attn (nn.Layer): 自注意力模块实例.\n",
    "            `MultiHeadedAttention` 的实例可以作为参数.\n",
    "        feed_forward (nn.Layer): Feed-forward 层的实例.\n",
    "            `PositionwiseFeedForward` 的实例可以作为参数.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "        normalize_before:\n",
    "             True: 在 sub-block前使用 layer-norm.\n",
    "             False: 在 sub-block后使用 layer-norm.\n",
    "        concat_after (bool): 是否合并attention层的输入和输出\n",
    "                True: x -> x + linear(concat(x, att(x)))\n",
    "                False: x -> x + att(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            size: int,\n",
    "            self_attn: nn.Layer,\n",
    "            src_attn: nn.Layer,\n",
    "            feed_forward: nn.Layer,\n",
    "            dropout_rate: float,\n",
    "            normalize_before: bool=True,\n",
    "            concat_after: bool=False, ):\n",
    "        \"\"\"构建 DecoderLayer 对象.\"\"\"\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.norm1 = nn.LayerNorm(size, epsilon=1e-12)\n",
    "        self.norm2 = nn.LayerNorm(size, epsilon=1e-12)\n",
    "        self.norm3 = nn.LayerNorm(size, epsilon=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.normalize_before = normalize_before\n",
    "        self.concat_after = concat_after\n",
    "        self.concat_linear1 = nn.Linear(size + size, size)\n",
    "        self.concat_linear2 = nn.Linear(size + size, size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            tgt: paddle.Tensor,\n",
    "            tgt_mask: paddle.Tensor,\n",
    "            memory: paddle.Tensor,\n",
    "            memory_mask: paddle.Tensor,\n",
    "            cache: Optional[paddle.Tensor]=None\n",
    "    ) -> Tuple[paddle.Tensor, paddle.Tensor, paddle.Tensor, paddle.Tensor]:\n",
    "        \"\"\"Compute decoded features.\n",
    "        Args:\n",
    "            tgt (paddle.Tensor): 输入tensor (#batch, maxlen_out, size).\n",
    "            tgt_mask (paddle.Tensor): 输入tensor的Mask\n",
    "                (#batch, maxlen_out).\n",
    "            memory (paddle.Tensor): Encoder的输出\n",
    "                (#batch, maxlen_in, size).\n",
    "            memory_mask (paddle.Tensor): Encoder的输出的mask\n",
    "                (#batch, maxlen_in).\n",
    "            cache (paddle.Tensor): 缓存 tensors.\n",
    "                (#batch, maxlen_out - 1, size).\n",
    "        Returns:   \n",
    "            x: paddle.Tensor: 输出tensor (#batch, maxlen_out, size).\n",
    "            tgt_mask: paddle.Tensor: 输出tensor的mask (#batch, maxlen_out).\n",
    "            memory: paddle.Tensor: Encoder的输出，这里输入和输出一致 (#batch, maxlen_in, size).\n",
    "            memory_mask: paddle.Tensor: Encoder的输出的mask (#batch, maxlen_in).\n",
    "        \"\"\"\n",
    "        residual = tgt\n",
    "        if self.normalize_before:\n",
    "            tgt = self.norm1(tgt)\n",
    "\n",
    "        if cache is None:\n",
    "            tgt_q = tgt\n",
    "            tgt_q_mask = tgt_mask\n",
    "        else:\n",
    "            # 使用最后一帧作为Q\n",
    "            assert cache.shape == [\n",
    "                tgt.shape[0],\n",
    "                tgt.shape[1] - 1,\n",
    "                self.size,\n",
    "            ], f\"{cache.shape} == {[tgt.shape[0], tgt.shape[1] - 1, self.size]}\"\n",
    "            tgt_q = tgt[:, -1:, :]\n",
    "            residual = residual[:, -1:, :]\n",
    "            tgt_q_mask = tgt_mask.cast(paddle.int64)[:, -1:, :].cast(\n",
    "                paddle.bool)\n",
    "\n",
    "        if self.concat_after:\n",
    "            tgt_concat = paddle.cat(\n",
    "                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1)\n",
    "            x = residual + self.concat_linear1(tgt_concat)\n",
    "        else:\n",
    "            x = residual + self.dropout(\n",
    "                self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))\n",
    "        if not self.normalize_before:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.norm2(x)\n",
    "        if self.concat_after:\n",
    "            x_concat = paddle.cat(\n",
    "                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)\n",
    "            x = residual + self.concat_linear2(x_concat)\n",
    "        else:\n",
    "            x = residual + self.dropout(\n",
    "                self.src_attn(x, memory, memory, memory_mask))\n",
    "        if not self.normalize_before:\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.norm3(x)\n",
    "        x = residual + self.dropout(self.feed_forward(x))\n",
    "        if not self.normalize_before:\n",
    "            x = self.norm3(x)\n",
    "\n",
    "        if cache is not None:\n",
    "            x = paddle.cat([cache, x], dim=1)\n",
    "\n",
    "        return x, tgt_mask, memory, memory_mask\n",
    "```\n",
    "\n",
    "#### Decoder解码的过程代码\n",
    "\n",
    "```python\n",
    " def recognize(\n",
    "            self,\n",
    "            speech: paddle.Tensor,\n",
    "            speech_lengths: paddle.Tensor,\n",
    "            beam_size: int=10,\n",
    "            decoding_chunk_size: int=-1,\n",
    "            num_decoding_left_chunks: int=-1,\n",
    "            simulate_streaming: bool=False, ) -> paddle.Tensor:\n",
    "        \"\"\" Apply beam search on attention decoder\n",
    "        Args:\n",
    "            speech (paddle.Tensor): (batch, max_len, feat_dim)\n",
    "            speech_length (paddle.Tensor): (batch, )\n",
    "            beam_size (int): beam size，top-k中k的值\n",
    "            decoding_chunk_size (int): 解码的chunk长度\n",
    "                <0: for decoding, 使用整句话\n",
    "                >0: for decoding, 使用decoding_chunk_size的长度.\n",
    "                0: 用于训练, 这里不能使用\n",
    "            simulate_streaming (bool): Encoder的输出是否按照流式\n",
    "        Returns:\n",
    "            best_hyps: paddle.Tensor: 解码结果 (batch, max_result_len)\n",
    "        \"\"\"\n",
    "        assert speech.shape[0] == speech_lengths.shape[0]\n",
    "        assert decoding_chunk_size != 0\n",
    "        device = speech.place\n",
    "        batch_size = speech.shape[0]\n",
    "\n",
    "        # Let's assume B = batch_size and N = beam_size\n",
    "        \n",
    "        # 1. 得到Encoder的结果\n",
    "        encoder_out, encoder_mask = self._forward_encoder(\n",
    "            speech, speech_lengths, decoding_chunk_size,\n",
    "            num_decoding_left_chunks,\n",
    "            simulate_streaming)  # (B, maxlen, encoder_dim)\n",
    "        maxlen = encoder_out.shape[1]\n",
    "        encoder_dim = encoder_out.shape[2]\n",
    "        running_size = batch_size * beam_size\n",
    "        encoder_out = encoder_out.unsqueeze(1).repeat(1, beam_size, 1, 1).view(\n",
    "            running_size, maxlen, encoder_dim)  # (B*N, maxlen, encoder_dim)\n",
    "        encoder_mask = encoder_mask.unsqueeze(1).repeat(\n",
    "            1, beam_size, 1, 1).view(running_size, 1,\n",
    "                                     maxlen)  # (B*N, 1, max_len)\n",
    "\n",
    "        hyps = paddle.ones(\n",
    "            [running_size, 1], dtype=paddle.long).fill_(self.sos)  # (B*N, 1)\n",
    "        \n",
    "        # log scale score\n",
    "        scores = paddle.to_tensor(\n",
    "            [0.0] + [-float('inf')] * (beam_size - 1), dtype=paddle.float)\n",
    "        scores = scores.to(device).repeat(batch_size).unsqueeze(1).to(\n",
    "            device)  # (B*N, 1)\n",
    "        end_flag = paddle.zeros_like(scores, dtype=paddle.bool)  # (B*N, 1)\n",
    "        cache: Optional[List[paddle.Tensor]] = None\n",
    "        \n",
    "        # 2. Decoder进行一步一步地解码\n",
    "        for i in range(1, maxlen + 1):\n",
    "            # Stop if all batch and all beam produce eos\n",
    "            if end_flag.cast(paddle.int64).sum() == running_size:\n",
    "                break\n",
    "\n",
    "            # 2.1 Decoder进行一次前向激素哪\n",
    "            hyps_mask = subsequent_mask(i).unsqueeze(0).repeat(\n",
    "                running_size, 1, 1).to(device)  # (B*N, i, i)\n",
    "            # logp: (B*N, vocab)\n",
    "            logp, cache = self.decoder.forward_one_step(\n",
    "                encoder_out, encoder_mask, hyps, hyps_mask, cache)\n",
    "\n",
    "            # 2.2 First beam prune: 选择当前step中topk个候选字符\n",
    "            top_k_logp, top_k_index = logp.topk(beam_size)  # (B*N, N)\n",
    "            top_k_logp = mask_finished_scores(top_k_logp, end_flag)\n",
    "            top_k_index = mask_finished_preds(top_k_index, end_flag, self.eos)\n",
    "\n",
    "            # 2.3 Seconde beam prune: 利用历史候选句子和当前step中的topk个候选进行组合，并选择当前step下topk个候选句子\n",
    "            scores = scores + top_k_logp  # (B*N, N), broadcast add\n",
    "            scores = scores.view(batch_size, beam_size * beam_size)  # (B, N*N)\n",
    "            scores, offset_k_index = scores.topk(k=beam_size)  # (B, N)\n",
    "            scores = scores.view(-1, 1)  # (B*N, 1)\n",
    "\n",
    "            # 2.4. 计算出topk个候选句子的index,\n",
    "            # 把 top_k_index 看成 (B*N*N)的形式,把 offset_k_index 看成 (B*N)的形式,\n",
    "            # 在 top_k_index 中找到 offset_k_index\n",
    "            base_k_index = paddle.arange(batch_size).view(-1, 1).repeat(\n",
    "                1, beam_size)  # (B, N)\n",
    "            base_k_index = base_k_index * beam_size * beam_size\n",
    "            best_k_index = base_k_index.view(-1) + offset_k_index.view(\n",
    "                -1)  # (B*N)\n",
    "\n",
    "            # 2.5 更新候选\n",
    "            best_k_pred = paddle.index_select(\n",
    "                top_k_index.view(-1), index=best_k_index, axis=0)  # (B*N)\n",
    "            best_hyps_index = best_k_index // beam_size\n",
    "            last_best_k_hyps = paddle.index_select(\n",
    "                hyps, index=best_hyps_index, axis=0)  # (B*N, i)\n",
    "            hyps = paddle.cat(\n",
    "                (last_best_k_hyps, best_k_pred.view(-1, 1)),\n",
    "                dim=1)  # (B*N, i+1)\n",
    "\n",
    "            # 2.6 更新 end flag\n",
    "            end_flag = paddle.eq(hyps[:, -1], self.eos).view(-1, 1)\n",
    "\n",
    "        # 3. 从top-k(beam-size)个最优结果中选择最优的结果\n",
    "        scores = scores.view(batch_size, beam_size)\n",
    "        best_index = paddle.argmax(scores, axis=-1).long()  # (B)\n",
    "        best_hyps_index = best_index + paddle.arange(\n",
    "            batch_size, dtype=paddle.long) * beam_size\n",
    "        best_hyps = paddle.index_select(hyps, index=best_hyps_index, axis=0)\n",
    "        best_hyps = best_hyps[:, 1:]\n",
    "        return best_hyps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_conf cmvn_file: None\n",
      "cmvn_file_type: json\n",
      "decoder: transformer\n",
      "decoder_conf:\n",
      "  attention_heads: 4\n",
      "  dropout_rate: 0.1\n",
      "  linear_units: 2048\n",
      "  num_blocks: 6\n",
      "  positional_dropout_rate: 0.1\n",
      "  self_attention_dropout_rate: 0.0\n",
      "  src_attention_dropout_rate: 0.0\n",
      "encoder: transformer\n",
      "encoder_conf:\n",
      "  attention_dropout_rate: 0.0\n",
      "  attention_heads: 4\n",
      "  dropout_rate: 0.1\n",
      "  input_layer: conv2d\n",
      "  linear_units: 2048\n",
      "  normalize_before: True\n",
      "  num_blocks: 12\n",
      "  output_size: 256\n",
      "  positional_dropout_rate: 0.1\n",
      "input_dim: 80\n",
      "model_conf:\n",
      "  ctc_dropoutrate: 0.0\n",
      "  ctc_grad_norm_type: None\n",
      "  ctc_weight: 0.3\n",
      "  length_normalized_loss: False\n",
      "  lsm_weight: 0.1\n",
      "output_dim: 4233\n",
      "2021-11-30 11:22:29.848 | INFO     | paddlespeech.s2t.models.u2.u2:_init_from_config:880 - U2 Encoder type: transformer\n",
      "2021-11-30 11:22:30.089 | INFO     | paddlespeech.s2t.modules.loss:__init__:41 - CTCLoss Loss reduction: sum, div-bs: True\n",
      "2021-11-30 11:22:30.090 | INFO     | paddlespeech.s2t.modules.loss:__init__:42 - CTCLoss Grad Norm Type: None\n",
      "2021-11-30 11:22:30.091 | INFO     | paddlespeech.s2t.modules.loss:__init__:73 - CTCLoss() kwargs:{'norm_by_times': False}, not support: {'norm_by_batchsize': False, 'norm_by_total_logits_len': False}\n"
     ]
    }
   ],
   "source": [
    "model_conf = transformer_config.model\n",
    "# input_dim 存储的是特征的纬度\n",
    "model_conf.input_dim = 80\n",
    "# output_dim 存储的字表的长度\n",
    "model_conf.output_dim = 4233 \n",
    "print (\"model_conf\", model_conf)\n",
    "model = U2Model.from_config(model_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载预训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dict = paddle.load(checkpoint_path)\n",
    "model.set_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 11:22:30.600 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:228 - BLANK id: 0\n",
      "2021-11-30 11:22:30.601 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:229 - UNK id: 1\n",
      "2021-11-30 11:22:30.602 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:230 - EOS id: 4232\n",
      "2021-11-30 11:22:30.602 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:231 - SOS id: 4232\n",
      "2021-11-30 11:22:30.602 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:232 - SPACE id: -1\n",
      "2021-11-30 11:22:30.602 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:233 - MASKCTC id: -1\n",
      "预测结果和对应的token id为:\n",
      "(['夺得国际田联竞走世界杯男子二十公里竞走银牌'], [[826, 1237, 712, 3961, 2482, 3014, 2778, 3590, 16, 2493, 1781, 2487, 936, 70, 426, 274, 3827, 2778, 3590, 3875, 2341, 4232]])\n",
      "预测结果为:\n",
      "夺得国际田联竞走世界杯男子二十公里竞走银牌\n"
     ]
    }
   ],
   "source": [
    "decoding_config = transformer_config.decoding\n",
    "# text_feature = collate_fn_test.text_feature\n",
    "text_feature = TextFeaturizer(unit_type='char',\n",
    "                            vocab_filepath=transformer_config.collator.vocab_filepath)\n",
    "\n",
    "result_transcripts = model.decode(\n",
    "            audio_feature,\n",
    "            audio_len,\n",
    "            text_feature=text_feature,\n",
    "            decoding_method=decoding_config.decoding_method,\n",
    "            lang_model_path=decoding_config.lang_model_path,\n",
    "            beam_alpha=decoding_config.alpha,\n",
    "            beam_beta=decoding_config.beta,\n",
    "            beam_size=decoding_config.beam_size,\n",
    "            cutoff_prob=decoding_config.cutoff_prob,\n",
    "            cutoff_top_n=decoding_config.cutoff_top_n,\n",
    "            num_processes=decoding_config.num_proc_bsearch,\n",
    "            ctc_weight=decoding_config.ctc_weight,\n",
    "            decoding_chunk_size=decoding_config.decoding_chunk_size,\n",
    "            num_decoding_left_chunks=decoding_config.num_decoding_left_chunks,\n",
    "            simulate_streaming=decoding_config.simulate_streaming)\n",
    "print (\"预测结果和对应的token id为:\")\n",
    "print (result_transcripts)\n",
    "print (\"预测结果为:\")\n",
    "print (result_transcripts[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业 \n",
    "1. 使用开发模式安装 [PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech)  \n",
    "环境要求：docker, Ubuntu 16.04，root user。  \n",
    "命令： `pip install -e .`\n",
    "\n",
    "2. 跑通 example/aishell/s1 中的conformer模型，完成训练和预测。 \n",
    "\n",
    "3. 按照 example 的格式使用自己的数据集训练 ASR 模型。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 关注PaddleSpeech\n",
    "https://github.com/PaddlePaddle/PaddleSpeech/  \n",
    "您的关注是我们最大的动力。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 参考文献\n",
    "\n",
    "[1] Graves A, Fernández S, Gomez F, et al. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks[C]//Proceedings of the 23rd international conference on Machine learning. 2006: 369-376.\n",
    "\n",
    "[2] Graves A, Mohamed A, Hinton G. Speech recognition with deep recurrent neural networks[C]//2013 IEEE international conference on acoustics, speech and signal processing. Ieee, 2013: 6645-6649.\n",
    "\n",
    "[3] Hannun A, Case C, Casper J, et al. Deep speech: Scaling up end-to-end speech recognition[J]. arXiv preprint arXiv:1412.5567, 2014.\n",
    "\n",
    "[4] Amodei D, Ananthanarayanan S, Anubhai R, et al. Deep speech 2: End-to-end speech recognition in english and mandarin[C]//International conference on machine learning. PMLR, 2016: 173-182.\n",
    "\n",
    "[5] Chan W, Jaitly N, Le Q, et al. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition[C]//2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016: 4960-4964.\n",
    "\n",
    "[6] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008.\n",
    "\n",
    "[7] Zhang Q, Lu H, Sak H, et al. Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 7829-7833.\n",
    "\n",
    "[8] Gulati A, Qin J, Chiu C C, et al. Conformer: Convolution-augmented transformer for speech recognition[J]. arXiv preprint arXiv:2005.08100, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio\n"
     ]
    }
   ],
   "source": [
    "## 回退到原始目录\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用Deepspeech2语音识别模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用Deepspeech2进行语音识别的流程\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"work/source/deepspeech2_pipeline.png\"  />\n",
    "</div>\n",
    "\n",
    "Deepspeech2进行语音识别的流程如上图所示。其中声学提取模块一般使用linear特征，也就是将音频信息由时域转到频域后的信息，没有使用mel滤波。\n",
    "\n",
    "而对于deepspeech2语音识别模型，其主要分为2个部分，第一个部分是Encoder，第二个部分是CTC Decoder。\n",
    "\n",
    "声学特征会首先进入Encoder，获取特征编码。然后CTC Decoder会利用Encoder提取的特征编码得到预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Deepspeech2 模型结构\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"work/source/deepspeech2_architecture.png\"  />\n",
    "</div>\n",
    "\n",
    "### Encoder\n",
    "Encoder 主要采用了2层降采样的CNN（subsampling Convolution layer）和多层RNN（Recurrent Neural Network）层组成。\n",
    "\n",
    "![subsampling CNN](work/source/subsampling_cnn.png)\n",
    "\n",
    "其中降采样的CNN的结构如上。其主要用途在于扩大每一个step的感受野，减少模型输入的帧数。\n",
    "\n",
    "而多层RNN的作用在于获取语音的上下文信息，这样可以获得更加准确的信息，并一定程度上进行语义消歧。Deepspeech2的模型中，每个RNN的cell使用了GRU或者LSTM。\n",
    "\n",
    "而最后softmax层将特征向量映射到为一个字表长度的向量，向量中存储了当前step结果预测为字表中每个字的概率。\n",
    "\n",
    "### Decoder\n",
    "Decoder的作用主要是将Encoder输出的概率转换为最终的文字结果。由于Deepspeech2采用的是CTC的损失函数，因此模型使用了CTC Decoder。\n",
    "对于CTC Decoder，主要有两种形式，第一种形式CTC greedy search decoder，其是采用greedy的方式进行解码。第二种形式是 CTC prefix beam search decoder，其采用的是beam search的方式进行解码。\n",
    "\n",
    "#### CTC Greedy Search\n",
    "<div align=center>\n",
    "<img src=\"work/source/CTC_greedy_search.png\"  />\n",
    "</div>\n",
    "\n",
    "对于CTC Greedy Search的方式，其只有一个候选序列，并且在每个时间点选择概率最高的字符加入候选序列来更新该候选序列，最后这个候选序列就可以直接生成最终结果。\n",
    "\n",
    "#### CTC Beam Search\n",
    "CTC Beam Search的方式是有beam size个候选序列，并在每个时间点生成新的最好的beam size个候选序列。最后在beam size个候选序列中选择概率最高的序列生成最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage 0 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 安装 paddlespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: sentencepiece==0.1.96 from file:///home/aistudio/work/wheel_store/sentencepiece-0.1.96-cp37-cp37m-linux_x86_64.whl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.1.96)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlespeech in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.1.0a1)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.21.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (3.4.5)\n",
      "Requirement already satisfied: sox in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.4.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (7.1.2)\n",
      "Requirement already satisfied: textgrid in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.5)\n",
      "Requirement already satisfied: numba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.48.0)\n",
      "Requirement already satisfied: pyworld in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.3.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.9.0)\n",
      "Requirement already satisfied: timer in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.2.2)\n",
      "Requirement already satisfied: unidecode in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.8.0)\n",
      "Requirement already satisfied: typeguard in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.13.2)\n",
      "Requirement already satisfied: inflect in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (5.3.0)\n",
      "Requirement already satisfied: sentencepiece~=0.1.96 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.1.96)\n",
      "Requirement already satisfied: llvmlite in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.31.0)\n",
      "Requirement already satisfied: coverage in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (6.2)\n",
      "Requirement already satisfied: paddlespeech-feat in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.0.1a0)\n",
      "Requirement already satisfied: GPUtil in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.4.0)\n",
      "Requirement already satisfied: gpustat in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.6.0)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.0.0)\n",
      "Requirement already satisfied: g2pM in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.1.2.5)\n",
      "Requirement already satisfied: distro in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.6.3)\n",
      "Requirement already satisfied: pybind11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.8.1)\n",
      "Requirement already satisfied: jsonlines in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.0.0)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.42.1)\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.2.0)\n",
      "Requirement already satisfied: editdistance in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.6.0)\n",
      "Requirement already satisfied: librosa in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.7.2)\n",
      "Requirement already satisfied: yq in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.12.2)\n",
      "Requirement already satisfied: pypi-kenlm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.1.20210121)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.8)\n",
      "Requirement already satisfied: paddlespeech-ctcdecoders in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.0.2a0)\n",
      "Requirement already satisfied: phkit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.2.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (4.27.0)\n",
      "Requirement already satisfied: resampy==0.2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.2.2)\n",
      "Requirement already satisfied: snakeviz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.1.1)\n",
      "Requirement already satisfied: kaldiio in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.17.2)\n",
      "Requirement already satisfied: praatio~=4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (4.4.0)\n",
      "Requirement already satisfied: pypinyin in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.44.0)\n",
      "Requirement already satisfied: yacs in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.1.8)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (5.7.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.2.3)\n",
      "Requirement already satisfied: ConfigArgParse in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.5.3)\n",
      "Requirement already satisfied: loguru in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.5.3)\n",
      "Requirement already satisfied: soxbindings in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.2.3)\n",
      "Requirement already satisfied: webrtcvad in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.0.10)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (1.1.5)\n",
      "Requirement already satisfied: g2p-en in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (2.1.0)\n",
      "Requirement already satisfied: soundfile~=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.10.3.post1)\n",
      "Requirement already satisfied: pynvml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (8.0.4)\n",
      "Requirement already satisfied: nara-wpe in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech) (0.0.7)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (5.1.2)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.4.10)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.15.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (0.23)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (16.7.9)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.3.4)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (0.10.0)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlespeech) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sox->paddlespeech) (1.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from numba->paddlespeech) (56.2.0)\n",
      "Requirement already satisfied: cython>=0.24.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyworld->paddlespeech) (0.29)\n",
      "Requirement already satisfied: mock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlespeech-feat->paddlespeech) (4.0.3)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gpustat->paddlespeech) (7.352.0)\n",
      "Requirement already satisfied: blessings>=1.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gpustat->paddlespeech) (1.7)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacrebleu->paddlespeech) (0.4.4)\n",
      "Requirement already satisfied: regex in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacrebleu->paddlespeech) (2021.11.10)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacrebleu->paddlespeech) (0.8.9)\n",
      "Requirement already satisfied: portalocker in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sacrebleu->paddlespeech) (2.3.2)\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (3.8.2)\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (3.14.0)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (1.0.0)\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (1.1.1)\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (0.7.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (2.22.0)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlespeech) (0.8.53)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (0.24.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (4.4.2)\n",
      "Requirement already satisfied: joblib>=0.12 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (0.14.1)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from librosa->paddlespeech) (2.1.8)\n",
      "Requirement already satisfied: argcomplete>=1.8.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from yq->paddlespeech) (1.12.3)\n",
      "Requirement already satisfied: xmltodict>=0.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from yq->paddlespeech) (0.12.0)\n",
      "Requirement already satisfied: hanziconv in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from phkit->paddlespeech) (0.3.2)\n",
      "Requirement already satisfied: tornado>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from snakeviz->paddlespeech) (6.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (2.4.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlespeech) (2019.3)\n",
      "Requirement already satisfied: distance>=0.1.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from g2p-en->paddlespeech) (0.1.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from soundfile~=0.10->paddlespeech) (1.14.0)\n",
      "Requirement already satisfied: bottleneck in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from nara-wpe->paddlespeech) (1.3.2)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from nara-wpe->paddlespeech) (7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->paddlespeech) (3.6.0)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlespeech) (2.2.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlespeech) (0.6.1)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlespeech) (2.6.0)\n",
      "Requirement already satisfied: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlespeech) (2.11.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlespeech) (2.8.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlespeech) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlespeech) (0.16.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlespeech) (1.25.6)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlespeech) (0.18.0)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlespeech) (3.9.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->paddlespeech) (2.1.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from cffi>=1.0->soundfile~=0.10->paddlespeech) (2.19)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlespeech) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlespeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 准备工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/work\n",
      "/home/aistudio/work/workspace_asr_ds2\n"
     ]
    }
   ],
   "source": [
    "%cd ./work\n",
    "!mkdir -p ./workspace_asr_ds2\n",
    "%cd ./workspace_asr_ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 获取预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-30 11:22:36--  https://paddlespeech.bj.bcebos.com/s2t/aishell/asr0/ds2.model.tar.gz\n",
      "Resolving paddlespeech.bj.bcebos.com (paddlespeech.bj.bcebos.com)... 182.61.200.229, 182.61.200.195, 2409:8c04:1001:1002:0:ff:b001:368a\n",
      "Connecting to paddlespeech.bj.bcebos.com (paddlespeech.bj.bcebos.com)|182.61.200.229|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 297856913 (284M) [application/octet-stream]\n",
      "Saving to: ‘ds2.model.tar.gz’\n",
      "\n",
      "ds2.model.tar.gz    100%[===================>] 284.06M  80.4MB/s    in 4.3s    \n",
      "\n",
      "2021-11-30 11:22:40 (66.3 MB/s) - ‘ds2.model.tar.gz’ saved [297856913/297856913]\n",
      "\n",
      "conf/deepspeech2.yaml\n",
      "data/mean_std.json\n",
      "exp/bw/checkpoints/avg_1.pdparams\n",
      "data/lang_char/\n",
      "data/lang_char/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://paddlespeech.bj.bcebos.com/s2t/aishell/asr0/ds2.model.tar.gz\n",
    "!tar xzvf ds2.model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-30 11:22:44--  https://deepspeech.bj.bcebos.com/zh_lm/zh_giga.no_cna_cmn.prune01244.klm\n",
      "Resolving deepspeech.bj.bcebos.com (deepspeech.bj.bcebos.com)... 182.61.200.229, 182.61.200.195, 2409:8c04:1001:1002:0:ff:b001:368a\n",
      "Connecting to deepspeech.bj.bcebos.com (deepspeech.bj.bcebos.com)|182.61.200.229|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2953395058 (2.8G) [application/octet-stream]\n",
      "Saving to: ‘data/lm/zh_giga.no_cna_cmn.prune01244.klm’\n",
      "\n",
      "zh_giga.no_cna_cmn. 100%[===================>]   2.75G  93.3MB/s    in 30s     \n",
      "\n",
      "2021-11-30 11:23:14 (94.3 MB/s) - ‘data/lm/zh_giga.no_cna_cmn.prune01244.klm’ saved [2953395058/2953395058]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!touch conf/augmentation.json\n",
    "# 下载语言模型\n",
    "!mkdir -p data/lm\n",
    "!wget -nc https://deepspeech.bj.bcebos.com/zh_lm/zh_giga.no_cna_cmn.prune01244.klm -P data/lm\n",
    "# 获取用于预测的音频文件\n",
    "%cp ../data/BAC009S0908W0355.wav ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 导入python包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from paddlespeech.s2t.exps.u2.config import get_cfg_defaults\n",
    "from paddlespeech.s2t.frontend.featurizer.text_featurizer import TextFeaturizer\n",
    "from paddlespeech.s2t.io.collator import SpeechCollator\n",
    "from paddlespeech.s2t.models.ds2 import DeepSpeech2Model\n",
    "from paddlespeech.s2t.utils import layer_tools\n",
    "\n",
    "#from yacs.config import CfgNode\n",
    "from paddlespeech.s2t.frontend.featurizer.audio_featurizer import AudioFeaturizer\n",
    "from paddlespeech.s2t.frontend.speech import SpeechSegment\n",
    "from paddlespeech.s2t.frontend.normalizer import FeatureNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 设置预训练模型的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Config========\n",
      "collator:\n",
      "  augmentation_config: conf/augmentation.json\n",
      "  batch_size: 64\n",
      "  delta_delta: False\n",
      "  dither: 1.0\n",
      "  feat_dim: None\n",
      "  keep_transcription_text: False\n",
      "  max_freq: None\n",
      "  mean_std_filepath: data/mean_std.json\n",
      "  n_fft: None\n",
      "  num_workers: 2\n",
      "  random_seed: 0\n",
      "  shuffle_method: batch_shuffle\n",
      "  sortagrad: True\n",
      "  spectrum_type: linear\n",
      "  spm_model_prefix: None\n",
      "  stride_ms: 10.0\n",
      "  target_dB: -20\n",
      "  target_sample_rate: 16000\n",
      "  unit_type: char\n",
      "  use_dB_normalization: True\n",
      "  vocab_filepath: data/lang_char/vocab.txt\n",
      "  window_ms: 20.0\n",
      "data:\n",
      "  dev_manifest: data/manifest.dev\n",
      "  manifest: \n",
      "  max_input_len: 27.0\n",
      "  max_output_input_ratio: inf\n",
      "  max_output_len: inf\n",
      "  min_input_len: 0.0\n",
      "  min_output_input_ratio: 0.0\n",
      "  min_output_len: 0.0\n",
      "  test_manifest: data/manifest.test\n",
      "  train_manifest: data/manifest.train\n",
      "decoding:\n",
      "  alpha: 1.9\n",
      "  batch_size: 128\n",
      "  beam_size: 300\n",
      "  beta: 5.0\n",
      "  ctc_weight: 0.0\n",
      "  cutoff_prob: 0.99\n",
      "  cutoff_top_n: 40\n",
      "  decoding_chunk_size: -1\n",
      "  decoding_method: ctc_beam_search\n",
      "  error_rate_type: cer\n",
      "  lang_model_path: data/lm/zh_giga.no_cna_cmn.prune01244.klm\n",
      "  num_decoding_left_chunks: -1\n",
      "  num_proc_bsearch: 10\n",
      "  simulate_streaming: False\n",
      "model:\n",
      "  blank_id: 0\n",
      "  cmvn_file: \n",
      "  cmvn_file_type: json\n",
      "  ctc_grad_norm_type: instance\n",
      "  decoder: transformer\n",
      "  decoder_conf:\n",
      "    attention_heads: 4\n",
      "    dropout_rate: 0.1\n",
      "    linear_units: 2048\n",
      "    num_blocks: 6\n",
      "    positional_dropout_rate: 0.1\n",
      "    self_attention_dropout_rate: 0.0\n",
      "    src_attention_dropout_rate: 0.0\n",
      "  encoder: transformer\n",
      "  encoder_conf:\n",
      "    attention_dropout_rate: 0.0\n",
      "    attention_heads: 4\n",
      "    dropout_rate: 0.1\n",
      "    input_layer: conv2d\n",
      "    linear_units: 2048\n",
      "    normalize_before: True\n",
      "    num_blocks: 12\n",
      "    output_size: 256\n",
      "    positional_dropout_rate: 0.1\n",
      "  input_dim: 0\n",
      "  model_conf:\n",
      "    ctc_weight: 0.3\n",
      "    length_normalized_loss: False\n",
      "    lsm_weight: 0.1\n",
      "  num_conv_layers: 2\n",
      "  num_rnn_layers: 3\n",
      "  output_dim: 0\n",
      "  rnn_layer_size: 1024\n",
      "  share_rnn_weights: False\n",
      "  use_gru: True\n",
      "training:\n",
      "  accum_grad: 1\n",
      "  checkpoint:\n",
      "    kbest_n: 50\n",
      "    latest_n: 5\n",
      "  global_grad_clip: 3.0\n",
      "  log_interval: 100\n",
      "  lr: 0.002\n",
      "  lr_decay: 0.83\n",
      "  n_epoch: 80\n",
      "  optim: adam\n",
      "  optim_conf:\n",
      "    lr: 0.0005\n",
      "    weight_decay: 1e-06\n",
      "  scheduler: warmuplr\n",
      "  scheduler_conf:\n",
      "    lr_decay: 1.0\n",
      "    warmup_steps: 25000\n",
      "  weight_decay: 1e-06\n"
     ]
    }
   ],
   "source": [
    "config_path = \"conf/deepspeech2.yaml\" \n",
    "checkpoint_path = \"./exp/bw/checkpoints/avg_1.pdparams\"\n",
    "audio_file = \"data/BAC009S0908W0355.wav\"\n",
    "\n",
    "result_file = \"exp/result.rsl\"\n",
    "\n",
    "# 读取 conf 文件并结构化\n",
    "ds2_config = get_cfg_defaults()\n",
    "ds2_config.merge_from_file(config_path)\n",
    "\n",
    "\n",
    "#ds2_config = CfgNode(new_allowed=True)\n",
    "#ds2_config.merge_from_file(config_path)\n",
    "print(\"========Config========\")\n",
    "print(ds2_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建音频特征提取对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_config = ds2_config.collator\n",
    "audio_featurizer = AudioFeaturizer(\n",
    "    spectrum_type=feat_config.spectrum_type,\n",
    "    feat_dim=feat_config.feat_dim,\n",
    "    delta_delta=feat_config.delta_delta,\n",
    "    stride_ms=feat_config.stride_ms,\n",
    "    window_ms=feat_config.window_ms,\n",
    "    n_fft=feat_config.n_fft,\n",
    "    max_freq=feat_config.max_freq,\n",
    "    target_sample_rate=feat_config.target_sample_rate,\n",
    "    use_dB_normalization=feat_config.use_dB_normalization,\n",
    "    target_dB=feat_config.target_dB,\n",
    "    dither=feat_config.dither)\n",
    "feature_normalizer = FeatureNormalizer(feat_config.mean_std_filepath) if feat_config.mean_std_filepath else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 提取音频的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Feature========\n",
      "Tensor(shape=[1, 849, 161], dtype=float32, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [[[ 1.33949280,  1.30350459, -0.04746983, ...,  1.59237695,\n",
      "           0.42001620, -1.61082637],\n",
      "         [-1.21325135,  0.12596609, -0.61995435, ...,  0.71308112,\n",
      "           0.58741844,  0.07226342],\n",
      "         [ 1.03332150,  0.87029517, -0.45239139, ...,  0.99254310,\n",
      "          -0.03513453,  1.27926254],\n",
      "         ...,\n",
      "         [-0.35289204,  0.27512935,  0.10877325, ...,  0.80169225,\n",
      "           0.63553405,  1.57065392],\n",
      "         [ 0.59620470,  0.19263259, -0.29943508, ...,  0.70254236,\n",
      "           1.09836316,  1.04222035],\n",
      "         [ 1.23617208,  1.17976248, -0.31874165, ..., -1.26211381,\n",
      "           0.13160947,  0.56485575]]])\n",
      "Tensor(shape=[1], dtype=int64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [849])\n"
     ]
    }
   ],
   "source": [
    "# 'None' 只是一个占位符，因为预测的时候不需要reference\n",
    "speech_segment = SpeechSegment.from_file(\n",
    "                audio_file, \"None\")\n",
    "audio_feature = audio_featurizer.featurize(speech_segment)\n",
    "if feature_normalizer:\n",
    "    audio_feature = feature_normalizer.apply(audio_feature)\n",
    "\n",
    "#audio_feature, _ = collate_fn_test.process_utterance(audio_file=audio_file, transcript=\"None\")\n",
    "#vocab_list = collate_fn_test.vocab_list\n",
    "print(\"========Feature========\")\n",
    "\n",
    "audio_len = audio_feature.shape[0]\n",
    "audio_feature = paddle.to_tensor(audio_feature, dtype='float32')\n",
    "audio_len = paddle.to_tensor(audio_len)\n",
    "audio_feature = paddle.unsqueeze(audio_feature, axis=0)\n",
    "print (audio_feature)\n",
    "print (audio_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构建transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 11:24:09.548 | INFO     | paddlespeech.s2t.modules.loss:__init__:41 - CTCLoss Loss reduction: sum, div-bs: True\n",
      "2021-11-30 11:24:09.549 | INFO     | paddlespeech.s2t.modules.loss:__init__:42 - CTCLoss Grad Norm Type: instance\n",
      "2021-11-30 11:24:09.550 | INFO     | paddlespeech.s2t.modules.loss:__init__:73 - CTCLoss() kwargs:{'norm_by_times': True}, not support: {'norm_by_batchsize': False, 'norm_by_total_logits_len': False}\n"
     ]
    }
   ],
   "source": [
    "model_conf = ds2_config.model\n",
    "# input dim is feature size\n",
    "model_conf.input_dim = 161\n",
    "# output_dim is vocab size\n",
    "model_conf.output_dim = 4301\n",
    "model = DeepSpeech2Model.from_config(model_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载预训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dict = paddle.load(checkpoint_path)\n",
    "model.set_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding_config alpha: 1.9\n",
      "batch_size: 128\n",
      "beam_size: 300\n",
      "beta: 5.0\n",
      "ctc_weight: 0.0\n",
      "cutoff_prob: 0.99\n",
      "cutoff_top_n: 40\n",
      "decoding_chunk_size: -1\n",
      "decoding_method: ctc_beam_search\n",
      "error_rate_type: cer\n",
      "lang_model_path: data/lm/zh_giga.no_cna_cmn.prune01244.klm\n",
      "num_decoding_left_chunks: -1\n",
      "num_proc_bsearch: 10\n",
      "simulate_streaming: False\n",
      "2021-11-30 11:24:15.669 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:228 - BLANK id: 0\n",
      "2021-11-30 11:24:15.670 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:229 - UNK id: 1\n",
      "2021-11-30 11:24:15.670 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:230 - EOS id: 4300\n",
      "2021-11-30 11:24:15.671 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:231 - SOS id: 4300\n",
      "2021-11-30 11:24:15.671 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:232 - SPACE id: -1\n",
      "2021-11-30 11:24:15.671 | INFO     | paddlespeech.s2t.frontend.featurizer.text_featurizer:_load_vocabulary_from_file:233 - MASKCTC id: -1\n",
      "2021-11-30 11:24:15.671 | INFO     | paddlespeech.s2t.modules.ctc:_init_ext_scorer:172 - begin to initialize the external scorer for decoding\n",
      "2021-11-30 11:24:15.858 | INFO     | paddlespeech.s2t.modules.ctc:_init_ext_scorer:182 - language model: is_character_based = 1, max_order = 5, dict_size = 0\n",
      "2021-11-30 11:24:15.859 | INFO     | paddlespeech.s2t.modules.ctc:_init_ext_scorer:183 - end initializing scorer\n",
      "['夺得国际田联竞走世界杯男子二十公里竞走银牌']\n",
      "预测结果为:\n",
      "夺得国际田联竞走世界杯男子二十公里竞走银牌\n"
     ]
    }
   ],
   "source": [
    "decoding_config = ds2_config.decoding\n",
    "print (\"decoding_config\", decoding_config)\n",
    "# text_feature = collate_fn_test.text_feature\n",
    "text_feature = TextFeaturizer(unit_type='char',\n",
    "                            vocab_filepath=ds2_config.collator.vocab_filepath)\n",
    "\n",
    "\n",
    "result_transcripts = model.decode(\n",
    "        audio_feature,\n",
    "        audio_len,\n",
    "        text_feature.vocab_list,\n",
    "        decoding_method=decoding_config.decoding_method,\n",
    "        lang_model_path=decoding_config.lang_model_path,\n",
    "        beam_alpha=decoding_config.alpha,\n",
    "        beam_beta=decoding_config.beta,\n",
    "        beam_size=decoding_config.beam_size,\n",
    "        cutoff_prob=decoding_config.cutoff_prob,\n",
    "        cutoff_top_n=decoding_config.cutoff_top_n,\n",
    "        num_processes=decoding_config.num_proc_bsearch)\n",
    "\n",
    "print (result_transcripts)\n",
    "print (\"预测结果为:\")\n",
    "print (result_transcripts[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
